%


\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,amssymb}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf EE502 - Linear Systems Theory II
	\hfill Spring 2032} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1 \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill } }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1}{Lecture #1}

   \vspace*{4mm}
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}


\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\newtheorem{exmp}[theorem]{Ex}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}

% Lecture Details
\lecture{6}{Assoc. Prof. M. Mert Ankarali}


\section{Modal Decomposition of State-Space Models}

\subsection{Zero Input Response}

Let's consider autonomous LTI CT and DT state-space models

In linear systems theory course, we are interested in matrix polynomials, specifically
%
\begin{align*}
       \dot{x} &= A x
       \\
	x[k+1] &= A x[k]
\end{align*}
%
Let $x_0 = \alpha v_i$, where $v_i$ is an eigenvector of $A$ associated with eigenvalue $\lambda_i$, we can 
then find the solution for both systems
%
\begin{align*}
	x(t) &= e^{A t} x_0 = \alpha e^{\lambda_i t} v_i
       \\
	x[k] &= A^k x_0 = \alpha \lambda_i^{k} v_i
\end{align*}
%
Now let's assume that $A$ is diagonalizable, then we now that there exist a set of $n$ linearly independent eigenvectors 
$\mathcal{V} = \lbrace v_i  \ , \ \cdots \ , \ v_n \rbrace$. Thus, we can write any initial condition, $x_0 \in \mathbb{R}^n$,
as a linear combination of eigenvectors, i.e.
%
\begin{align*}
	x_0 = \sum\limits_{i=1}^n \alpha_i v_i
\end{align*}
%
if we use this (modal) decomposition we can find the solutions of DT and CT equations as
%
%
\begin{align*}
	x(t) &= e^{A t} x_0 = \sum\limits_{i=1}^n \alpha_i e^{\lambda_i t} v_i
       \\
	x[k] &= A^k x_0 = \sum\limits_{i=1}^n \alpha_i \lambda_i^{k} v_i
\end{align*}
%
where $e^{\lambda_i t} v_i$ ($\lambda_i^{k} v_i$ in DT case) is called a ``mode'' of the system. Now let's try to 
find $\lbrace \alpha_i \ ,  \ \cdots \ , \ \alpha_n \rbrace$ via diagonalizition of $A$
%
\begin{align*}
	A &= V \Lambda V^{-1} = V \left[ \begin{array}{ccccc} \lambda_1 &  & & &  \\  & \lambda_2  &  & 0 &  \\ &  & \ddots & \\ & 0 & & \ddots & \\ & &  & &  \lambda_n \end{array} \right] V^{-1} \ , \mathrm{where}
	\\
	 V &= \left[ \begin{array}{ccc} v_1 & \cdots & v_n \end{array} \right] \ , \ \mathrm{where} \ A v_i = \lambda_i v_i 
 \\
 V^{-1} &= \bar{V} =  \left[ \begin{array}{c} \bar{v}_1^T \\ \vdots \\ \bar{v}_n^T \end{array} \right] \ , \ \mathrm{where} \ \bar{v}_i^T A = \lambda_i \bar{v}_i^T
  \\
\bar{V} V &= V \bar{V} = I \ , \ \bar{v}_i^T v_i = 1 \ , \  \bar{v}_i^T v_j = 0 \ \mathrm{for} \, i \neq j
\end{align*}
%
Now let's compute the zero-input responses for an arbitrary $x_0$
%
%
\begin{align*}
	x(t) &= e^{A t} x_0 = V e^{\Lambda t} V^{-1} x_0 = \left[ \begin{array}{ccc} v_1 & \cdots v_n \end{array} \right] 
	\left[ \begin{array}{c} e^{\lambda_1 t} \bar{v}_1^T x_0 \\  \vdots \\ e^{\lambda_n t} \bar{v}_n^T x_0 \end{array} \right] 
	= \sum\limits_{i=1}^n v_i e^{\lambda_i t} \left( \bar{v}_i^T x_0 \right) \ \rightarrow \ \alpha_i = \bar{v}_i^T x_0
       \\
	x[k] &= V \Lambda^k V^{-1} x_0 = \left[ \begin{array}{ccc} v_1 & \cdots v_n \end{array} \right] 
	\left[ \begin{array}{c} \lambda_1^k \bar{v}_1^T x_0 \\  \vdots \\ \lambda_n^k \bar{v}_n^T x_0 \end{array} \right] 
	= \sum\limits_{i=1}^n v_i \lambda_i^k \left( \bar{v}_i^T x_0 \right) \ \rightarrow \ \alpha_i = \bar{v}_i^T x_0
\end{align*}
%
Now let's try to find a similar solution for systems where matrix $A$ can not be diagonalizable. For the sake of clarity let's focus on 
matrices that is composed of a single Jordan block, i.e.
%
\begin{align*}
A &= G J G^{-1} = G \left[  \begin{array}{ccccc} \lambda & 1 & 0 & \cdots & 0  \\ 0 & \lambda & 1 & 0 & \cdots  \\ 
\vdots &  & \ddots &  \\ & & & \ddots & 1 \\
0 &  &  & 0 & \lambda \end{array} \right] G^{-1} 
\end{align*}
%
\noindent where
%
\begin{align*}
	 G &= \left[ \begin{array}{ccc} g_1 & \cdots & g_n \end{array} \right]
	 \\
 A g_1 &= \lambda g_1 \ \rightarrow \ (A - \lambda I) g_1 = 0
	\\
	A g_2 &= \lambda g_2 + g_1 \ \rightarrow \ (A - \lambda I) g_2 = g_1 \ , \ \mathrm{note} \ (A - \lambda I)^2 g_2 = 0 \ \& \ (A - \lambda I) g_2 \neq 0
		\\
	A g_3 &= \lambda g_3 + g_2 \ \rightarrow \ (A - \lambda I) g_3 = g_2 \ , \ \mathrm{note} \ (A - \lambda I)^3 g_3 = 0 \ \& \ (A - \lambda I)^2 g_3 \neq 0
	\\
	&\vdots
	\\
	A g_{n} &= \lambda g_{n} + g_{n-1} \ \rightarrow \ (A - \lambda I) g_n = g_{n-1} \ , \ \mathrm{note} \ (A - \lambda I)^n g_n = 0 \ \& \ (A - \lambda I)^{n-1} g_{n} \neq 0
\end{align*}
and we also know that
\begin{align*}
 G^{-1} &= \bar{G} =  \left[ \begin{array}{c} \bar{g}_1^T \\ \vdots \\ \bar{g}_n^T \end{array} \right] 
  \\
\bar{G} G &= G \bar{G} = I \ , \ \bar{g}_i^T g_i = 1 \ , \  \bar{g}_i^T g_j = 0 \ \mathrm{for} \, i \neq j
\end{align*}
%
Let $x_0 = \alpha_1 g_1$, i.e. the eigenvector of $A$, then we can find the responses as
 %
\begin{align*}
	x(t) &= e^{A t} g_1 = G e^{J t} G^{-1} g_1 \alpha_1
	\\ &= \left[ \begin{array}{ccc} g_1 & \cdots g_n \end{array} \right] 
	%
\left[  \begin{array}{cccccc} e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t} 
& \cdots & \frac{t^{n-2}}{(n-2) !} e^{\lambda t}  & \frac{t^{n-1}}{(n-1) !} e^{\lambda t}
\\ 0 & e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t} & \cdots  & \frac{t^{n-2}}{(n-2) !} e^{\lambda t}
\\  \vdots &  & \ddots &  &  & \vdots \\ 
& & & e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t}
\\ 0 &  & \cdots  &  & e^{\lambda t} & t e^{\lambda t} \\
0 &  & \cdots &  & 0 & e^{\lambda t} \end{array} \right] 
%
	\left[ \begin{array}{c} \alpha_1 \\ 0 \\ \vdots \\ 0 \end{array} \right] 
	\\ &= \alpha_1 e^{\lambda t} g_1  
       \\
       \\
	x[k] &= G J^k G^{-1} x_0 = \alpha_1 \lambda^k g_1   
\end{align*}
%
the format of the solution associated with $g_1$ seems to be exactly same with diagonal case (since $g_1$ is an eigenvector). 
This implies that, if we choose an initial condition inside the eigenvector space, the response stays in this space and acts like a 
``first-order'' system. Now, let $x_0 = \alpha_2 g_2$, i.e. a first order generalized eigenvector of $A$, then we can find the responses as
 %
\begin{align*}
	x(t) &= G e^{J t} G^{-1} g_2 \alpha_2
	\\ &= \left[ \begin{array}{ccc} g_1 & g_2 & \cdots g_n \end{array} \right] 
	%
\left[  \begin{array}{cccccc} e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t} 
& \cdots & \frac{t^{n-2}}{(n-2) !} e^{\lambda t}  & \frac{t^{n-1}}{(n-1) !} e^{\lambda t}
\\ 0 & e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t} & \cdots  & \frac{t^{n-2}}{(n-2) !} e^{\lambda t}
\\  \vdots &  & \ddots &  &  & \vdots \\ 
& & & e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2 !} e^{\lambda t}
\\ 0 &  & \cdots  &  & e^{\lambda t} & t e^{\lambda t} \\
0 &  & \cdots &  & 0 & e^{\lambda t} \end{array} \right] 
%
	\left[ \begin{array}{c} 0 \\ \alpha_2 \\ \vdots \\ 0 \end{array} \right] 
	\\ 
	&= \left[ \begin{array}{ccc} g_1 & g_2 & \cdots g_n \end{array} \right] \left[ \begin{array}{c} \alpha_2 t e^{\lambda t} \\ \alpha_2 e^{\lambda t} \\ \vdots \\ 0 \end{array} \right] = \alpha_2 \left( t e^{\lambda t} g_1  + e^{\lambda t} g_2 \right)
\end{align*}
%
\begin{align*}
	x[k] &= G J^k G^{-1} g_2 \alpha_2 
	\\
	&= \left[ \begin{array}{ccc} g_1 & g_2 & \cdots g_n \end{array} \right] 
	%
\left[  \begin{array}{cccccc} \lambda^k & \begin{pmatrix} k \\ 1 \end{pmatrix} \lambda^{k-1} & \begin{pmatrix} k \\ 2 \end{pmatrix}  \lambda^{k-2} 
& \cdots & \begin{pmatrix} k \\ n\mathrm{-}2 \end{pmatrix} \lambda^{k-n+2} & \begin{pmatrix} k \\ n\mathrm{-}1 \end{pmatrix} \lambda^{k-n+1}
\\ 0 & \lambda^k & \begin{pmatrix} k \\ 1 \end{pmatrix}  \lambda^{k-1} & \begin{pmatrix} k \\ 2 \end{pmatrix} \lambda^{k-2} & \cdots  & \begin{pmatrix} k \\ n\mathrm{-}2 \end{pmatrix} \lambda^{k-n+2}
\\ 
\\  \vdots &  & \ddots &  &  & \vdots \\ 
\\ 
& & & \lambda^k & \begin{pmatrix} k \\ 1 \end{pmatrix}  \lambda^{k-1} & \begin{pmatrix} k \\ 2 \end{pmatrix}  \lambda^{k-2} 
\\ 0 &  & \cdots  &  & \lambda^k & \begin{pmatrix} k \\ 1 \end{pmatrix}  \lambda^{k-1} \\
0 &  & \cdots &  & 0 & \lambda^k \end{array} \right]  
%
	\left[ \begin{array}{c} 0 \\ \alpha_2 \\ \vdots \\ 0 \end{array} \right] 
	\\ 
	&= \left[ \begin{array}{ccc} g_1 & g_2 & \cdots g_n \end{array} \right] \left[ \begin{array}{c} \alpha_2 k \lambda^{k-1} \\ \alpha_2 \lambda^k \\ \vdots \\ 0 \end{array} \right] = \alpha_2 \left( k \lambda^{k-1} g_1  + \lambda^k g_2 \right)
\end{align*}
%
We can observe that the response acts like a ``second-order'' (critically-damped) response. Moreover, the response does not stays inside the span of the 
generalized eigenvector, i.e. $\mathrm{Span} \lbrace g_2 \rbrace$, instead it navigates inside the span of the eigenvector and $g_2$, i.e. 
$\mathrm{Span} \lbrace g_1 \ , \ g_2 \rbrace = \mathcal{N}(A - \lambda I)^2$. Now, let $x_0 = \alpha_i g_i \ , \ 0 \leq i \leq n$, i.e. order generalized eigenvector
of order $i$, then we can find the responses as
%
\begin{align*}
x(t) &= G e^{J t} G^{-1} g_2 \alpha_2 = \alpha_i e^{\lambda t} \sum\limits_{j=1}^{i}  g_j \frac{t^{i-j}}{(i-j)!}
\\
x[k] &= G J^k G^{-1} g_i \alpha_i = \alpha_i \sum\limits_{j=1}^{i}  g_j \begin{pmatrix} k \\ i-j \end{pmatrix} \lambda^{k-i+j}  
\end{align*}
%
Similar to the second-order case, we can see that response acts like an $i^{th}$ order dynamical system, and trajectories stays inside,
$\mathrm{Span} \lbrace g_1 \ , \ cdots \, \ g_i \rbrace = \mathcal{N}(A - \lambda I)^i$. In this context, in order to excite all higher order modes 
of the system projection of the initial condition to the sub-space spanned highest order generalized eigenvector has to be non-zero. Now, 
let's try to find general solution for an arbitrary $x_0$. We can write any $x_0 \in \mathbb{R}^{n}$ as a linear combination of 
$\mathcal{G} = \lbrace g_1 \ , \ g_2 \ , \ \cdots \ , \ g_n \rbrace$, thus we have
%
\begin{align*}
\\
x_0 &= \sum\limits_{i=1}^n \alpha_i g_i
\\
x(t) &= \sum\limits_{i=1}^n \alpha_i e^{\lambda t} \sum\limits_{j=1}^{i}  g_j \frac{t^{i-j}}{(i-j)!}
\\
x[k] &= \sum\limits_{i=1}^n \alpha_i \sum\limits_{j=1}^{i}  g_j \begin{pmatrix} k \\ i-j \end{pmatrix} \lambda^{k-i+j}  
\end{align*}
%
where 
\begin{align*}
\left[ \begin{array}{c} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{array} \right] = G^{-1} x_0 
= \left[ \begin{array}{c} \bar{g}_1^T \\ \bar{g}_2^T \\ \vdots \\ \bar{g}_n^T \end{array} \right] x_0 \ \rightarrow \ \alpha_i = \bar{g}_i^T x_0
\end{align*}

\begin{exmp}
	Let 
\begin{align*}
\dot{x} = A x \ \mathrm{where} \  A = \left[ \begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1
\end{array} \right] , 
\end{align*}
\end{exmp}
%
derive the solution of $x(t)$ using modal decomposition for an arbitrary $x_0 \in \mathbb{R}^3$ 

\textbf{Solution:} We know that Jordan canonical form of matrix $A$ has the form
%
\begin{align*}
  J = \left[ \begin{array}{ccc} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right] 
\end{align*}
% 
and the transformation matrices that leads to this Jordan form are
%
\begin{align*}
 G &= \left[ \begin{array}{cccc} g_1 & g_2 & v  \end{array} \right]
 \ , \ 
 G = \left[ \begin{array}{ccc} 1 & 0 & 1 \\ 1 & 0 & 0  \\ 0 & 1 & 0  \end{array} \right]
 \  , \ 
 G^{-1} = \left[ \begin{array}{ccc} 0 & 1 & 0 \\ 0 & 0 & 1  \\ 1 & -1 & 0 \end{array} \right]
\end{align*}
%
where $g_1$ and $v$ are eigenvectors and $g_2$ is the single generalized eigenvector associated with $g_1$.
If we follow the derivation in this lecture note, we can first find all individual components of the responses as
%
\begin{align*}
 x_{g_1}(t) &= \alpha_{g_1} e^{t} g_1  
 \\
 x_{g_2}(t) &= \alpha_{g_2} \left( t e^{t} g_1  + e^{t} g_2 \right)
 \\
 x_{v}(t) &= \alpha_{v} e^{t} v  
\end{align*}
%
where the combined solution and $\alpha_{*}$'s can be derived using
%
\begin{align*}
x(t) &=  x_{g_1}(t) +  x_{g_2}(t) +  x_{v}(t)  = e^{t} \left( (\alpha_{g_1} + t \alpha_{g_2})g_1 + \alpha_{g_2}  g_2 + \alpha_{v} v  \right) 
\\
\left[ \begin{array}{ccc} \alpha_{g_1} \\ \alpha_{g_2} \\ \alpha_{v} \end{array} \right] &= G^{-1} x_0 = \left[ \begin{array}{ccc} 0 & 1 & 0 \\ 0 & 0 & 1  \\ 1 & -1 & 0 \end{array} \right] x_0
\end{align*}


% **** This ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:
\end{document}
