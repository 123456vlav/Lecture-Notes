%


\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,amssymb}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf EE502 - Linear Systems Theory II
	\hfill Spring 2032} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1 \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill } }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1}{Lecture #1}

   \vspace*{4mm}
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}


\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\newtheorem{exmp}[theorem]{Ex}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}

% Lecture Details
\lecture{5}{Assoc. Prof. M. Mert Ankarali}


\section{Functions of a Matrix}

In linear systems theory course, we are interested in matrix polynomials, specifically
%
\begin{itemize}
	\item Matrix Exponential in CT Systems:  $e^{A t}$ 
	\item Matrix Power in DT Systems:  $A^k$ 	
\end{itemize}
%
which arise on the solution of state-space equations in their respective domains. 
Obviously $A^k$ in DT systems is ``easier'' to analyze and understand compared to matrix exponential. 
Let's first review the matrix exponential, $e^{A t}$.
Let $t \in \mathbb{R}$ and $A \in \mathbb{R}^{nxn}$, then $e^{At}$ defined as
%
\begin{align*}
       e^{A t} &:= I + A t + \frac{1}{2 !} A^2 t^2 + \frac{1}{3 !} A^3
                 t^3 + \cdots 
\\
	e^{A t} &:= \sum\limits_{k=0}^{\infty} \frac{t^k}{k!} A^k
\end{align*}
%
which converges for all  $t \in \mathbb{R}$ and $A \in
\mathbb{R}^{nxn}$.

Now let's review some properties

\begin{itemize}

 \item \textbf{Claim:} $ \frac{d}{d t} e^{A t} = A e^{A t} = e^{A t} A$

\textbf{Proof:} 
%
\begin{align*}
 	\frac{d}{dt} e^{A t} &= \frac{d}{dt} \left( \sum\limits_{k=0}^{\infty} \frac{t^{k}}{k!} A^k \right) =
	\sum\limits_{k=0}^{\infty} k \frac{t^{k-1}}{k!} A^k = \sum\limits_{k=1}^{\infty} \frac{t^{k-1}}{(k-1)!} A^k \\
	&= \sum\limits_{n=0}^{\infty} \frac{t^{n}}{n!} A^{n+1} = A \sum\limits_{n=0}^{\infty} \frac{t^{n}}{n!} A^{n}   = \left( \sum\limits_{n=0}^{\infty} \frac{t^{n}}{n!} A^{n} \right) A
	\\
	\frac{d}{dt} e^{A t}  &= A e^{A t} = e^{A t} A
\end{align*}

\item \textbf{Claim:} Let $A , B \in \mathrm{R}^{n \times n}$ and $A B
  = B A$, then 
%
\begin{align*}
	e^{A} e^{B} &= e^{B} e^{A} = e^{(A + B)} 
\end{align*}

\textbf{Proof:} 

\begin{align*}
	e^{A} e^{B} &= \left(\sum\limits_{k=0}^{\infty} \frac{1}{k!} A^k \right) \left(\sum\limits_{j=0}^{\infty} \frac{1}{j!} B^j \right) 
	= \sum\limits_{k=0}^{\infty} \sum\limits_{j=0}^{\infty} \frac{A^k}{k!} \frac{B^j}{j!} 
\end{align*}
%
Let $n = k + j$ and $j = n - k$, then
%
\begin{align*}
	e^{A} e^{B} &= \sum\limits_{k=0}^{\infty} \sum\limits_{n=k}^{\infty} \frac{A^k \ B^{n-k}}{k! (n-k)!} 
	= \sum\limits_{n=0}^{\infty} \sum\limits_{k=0}^{n} \frac{A^k \ B^{n-k}}{n!} \frac{n!}{k! (n-k)!}  
    = \sum\limits_{n=0}^{\infty} \frac{1}{n!} \sum\limits_{k=0}^{n} A^k \ B^{n-k}
    \left( \begin{array}{c} n \\ k \end{array} \right) 
\end{align*}
%
Note that if $A B \neq B A$ we has to stop at this point. However, since $A B = B A$, we can adopt binomial theorem 
%
\begin{align*}
	e^{A} e^{B} &= \sum\limits_{n=0}^{\infty} \frac{1}{n!} (A+B)^{n} = e^{A+B} = e^{B} e^{A}
\end{align*}

\item \textbf{Claim:} Let $t_1 , t_2 \in \mathrm{R}$ then
%
\begin{align*}
  e^{A t_1} e^{A t_2} = e^{A t_2} e^{A t_1} = e^{A \left( t_1 + t_2 \right)}
\end{align*}
%
\textbf{Proof:} Let $A := A t_1$ and $B := A t_2$, obviously $(A t_1) (A t_2) = (A t_2) (A t_1)$ hence we can use the 
previous property, i.e. 
%
\begin{align*}
e^{A t_1} e^{A t_2} = e^{ \left( A t_1 \right) +  \left( A  t_2 \right)}
= e^{A \left( t_1 + t_2 \right)} = e^{A t_2} e^{A t_1} 
%
\end{align*}
%
Now let $t_1 = t$ and $t_2 = -t$, then we have
%
\begin{align*}
	e^{A t} e^{-A t} &= e^{A (t - t)} = I \quad \rightarrow \quad
                           \left( e^{A t} \right)^{-1} = e^{-A t}
\end{align*}

\item \textbf{Claim:} Let $P \in \mathrm{R}^{n \times n}$ and
 $\mathrm{det}(P) \neq 0$, then
%
\begin{align*}
	e^{\left(P^{-1} A P \right) t} = P^{-1} e^{A t} P
\end{align*}
%
\textbf{Proof:} Let's firs show that $ \left(P^{-1} A P \right)^k  = P^{-1} A^k P $
%
\begin{align*}
	\left(P^{-1} A P \right)^k &= \left(P^{-1} A P \right) \left(P^{-1} A P \right) \cdots \left(P^{-1} A P \right) \left(P^{-1} A P \right)
	\\
	&= P^{-1} A P P^{-1} A P P^{-1} \cdots P P^{-1} A P P^{-1} A P 
	\\
	&= P^{-1} A I A I \cdots I A I A P 
	\\
	&= P^{-1} A^k P
\end{align*}
%
Now let's expand
%
\begin{align*}
	e^{\left(P^{-1} A P \right) t} &= \sum\limits_{k=0}^{\infty} \frac{t^k}{k!} \left(P^{-1} A P \right)^k
	\\
	&= \sum\limits_{k=0}^{\infty} \frac{t^k}{k!} P^{-1} A^k P 
	\\
	&= P^{-1} \left(  \sum\limits_{k=0}^{\infty} \frac{t^k}{k!}  A^k \right) P 
	\\
	&= P^{-1} e^{A t} P
\end{align*}

\end{itemize}

\subsection{Computation of $e^{At}$ and $A^k$}

\subsection{Computation via Solution of State-Space Equations and Frequency Domain Expressions}

An LTI CT state-space representation of an autonomous system has the form
%
\begin{align*}
  \dot{x}(t) &= A x(t) \ , \mathrm{where} \ x(t) \in \mathbb{R}^n 
\end{align*}
%
Let's test if $x(t) = e^{A t} x_0$ is a solution of the homogeneous  equation
%
\begin{align*}
  x(0) &= e^{A \, 0} x_0 = x_0
  \\
  \dot{x}(t) - A x(t) &= \left( A e^{At} \right) x_0 - A e^{At} x_0 = 0
\end{align*}
%
Now let's remember Laplace domain solution of the same equation
%
\begin{align*}
\mathcal{L} \left[ \dot{x}(t) \right] &= \mathcal{L} \left[  A x(t) \right]
\\
s X(s) - x(0) &= A X(s) 
\\
[s I - A]  X(s) &= x(0) 
\\
X(s) &= [s I - A]^{-1} x_0 
\end{align*}
%
If we connect time and s-domain solutions we obtain
%
\begin{align*}
  e^{A t} &= \mathcal{L}^{-1} \left[ [s I - A]^{-1} \right]
\end{align*}

Now let's focus on $A^k$. An LTI DT state-space representation of an autonomous system has the form
%
\begin{align*}
  x[k+1] &= A x[k] 
\end{align*}
%
Unlike CT systems we can compute the response iteratively easily
%
\begin{align*}
  x[1] &= G x[0] 
  \\
  x[2] &= G x[1] = G^2 x[0] 
  \\
  x[3] &= G x[2] = G^3 x[0] 
  \\
  \vdots
  \\
  x[k] &= G x[k-1] = G^k x[0] 
\end{align*}
%
Now let's remember form of the response in Z-domain.
%
\begin{align*}
\mathcal{Z} \left[ x[k+1] \right] &= \mathcal{Z} \left[ G x[k] \right]
\\
z X(z) - z x[0] &= G X(z)
\\
\left( z I - G \right) X(z) &= z X(z) 
\\
X(z) &= z \left( z I - G \right)^{-1} x[0]
\end{align*}
%
If we connect time and s-domain solutions we obtain
\begin{align*}
G^k &= \mathcal{Z}^{-1} \lbrace z \left( z I - G \right)^{-1} \rbrace
\end{align*}

\subsection{Computation via Diagonalization}

\textbf{Theorem:} $A \in \mathbb{C}^{n \times n}$ is diagonalizable, if and only if there exists a (nonsingular) similarity transformation, 
$V \in \mathbb{C}^{n \times n}$, such that $A = V \Lambda V^{-1}$ where $\Lambda$ is a diagonal matrix, 
%
\begin{align*}
	\Lambda = \left[ \begin{array}{ccccc} \lambda_1 &  & & &  \\  & \lambda_2  &  & 0 &  \\ &  & \ddots & \\ & 0 & & \ddots & \\ & &  & &  \lambda_n \end{array} \right]
\end{align*}
%
where $\lambda_i \in \mathbb{C}$'s are the eigenvalues of $A$, which are the roots of the characteristic equation $d(\lambda) = \mathrm{det}(\lambda I - A)$.

Now let's compute $A^k$ and $e^{At}$ for a diagonalizable matrix 
%
\begin{align*}
A^k &= \left(V^{-1} \Lambda V \right)^k  = V \Lambda^k V^{-1} = V \left[ \begin{array}{ccc} \lambda_1^k &  &   \\   &  \ddots & \\ & & \lambda_n^k \end{array} \right] V^{-1}
\\
e^{A t} &= e^{V^{-1} \Lambda V t}  = V e^{ \Lambda t} V^{-1} = V \left[ \begin{array}{ccc} e^{\lambda_1 t} &  &   \\   &  \ddots & \\ & & e^{\lambda_n t} \end{array} \right] V^{-1}  
\end{align*}
%
A sufficient but not necessary condition that $A$ will have $n$ distinct eigenvalues in such a case characteristic equation will have the following form
%
\begin{align*}
d(\lambda) = (\lambda - \lambda_1) \cdots (\lambda - \lambda_n) \ , \ \mathrm{where} \ \lambda_i \neq \lambda_j \ , \ \mathrm{if} \ i \neq j 
\end{align*}
% 
In this case, we also have the following properties associated with $A$
%
\begin{itemize}
	\item $A$ has $n$ linearly independent eigenvectors
	\item For each $\lambda_i$ there exists an eigenvector, $v_i$, such that $A v_i = \lambda v_i$ and $\mathrm{Span}\lbrace v_1 , \cdots , v_n \rbrace = \mathbb{C}^n$
	\item $\forall \ \lambda_i$, geometric multiplicity is equal to algebraic multiplicity and they are both equal to 1, i.e. $\mathrm{GM}(\lambda_i) = \mathrm{AM}(\lambda_i) = 1$
	\item Minimal polynomial is equal to the characteristic equation, $m(\lambda) = d(\lambda)$
	\item For each $\lambda_i$, $\mathcal{N}(\lambda_i I - A) = \mathrm{Span} \lbrace v_i \rbrace $ and $ \mathrm{dim} [ \mathcal{N}(\lambda_i I - A) ] = 1$
\end{itemize}
%
Let's remember some concepts from EE501, to better understand and generalize \textit{diagonalizable} and \textit{non-diagonalizable} square matrices.  

\textbf{Definition:} Given a matrix $A \in \mathbb{C}^{n \times n}$, the characteristic polynomial $d(\lambda)$ is defined as 
%
\begin{align*}
d(\lambda) &= (\lambda - \lambda_1)^{r_1} \cdots (\lambda - \lambda_k)^{r_k} 
\\
k &: \ \# \ \mathrm{distinct} \ \mathrm{eigenvalues}  \ \mathrm{where} \ k \leq n
\\
r_i = AM(\lambda_i) &: \ \# \ \mathrm{algebraic} \ \mathrm{multiplicty}  \ \lambda_i \ , \ \mathrm{where} \ n = \sum_{i=1}^k r_i
\end{align*}
% 
\textbf{Theorem:} Every $n \times n$ matrix satisfies its characteristic equation (Cayley-Hamilton Theorem)
%
\begin{align*}
d(\lambda) &= (\lambda - \lambda_1)^{r_1} \cdots (\lambda - \lambda_k)^{r_k} = \lambda^n + d_{n-1} \lambda^{n-1} + \cdots + d_0
\\
d(A) &= A^n + d_{n-1} A^{n-1} + \cdots + d_1 A + d_0 I = 0_{n \times n} 
\end{align*}
%
\textbf{Remark:} Any power of a matrix $A \in \mathbb{C}^{n \times n}$ can be written as a linear combination of 
$\mathcal{A}_n = \lbrace I \ , \ A \ , \ A^2 \ , \ \cdots \ , \ A^{n-1} \rbrace$.

Note that Cayley-Hamilton theorem does not guarantee that $\lbrace I \ , \ A \ , \ A^2 \ , \ \cdots \ , \ A^{n-1} \rbrace$ are linearly independent. 

\textbf{Definition:} For an $A \in \mathbb{R}^{n \times n}$, the minimal polynomial $m(\lambda)$ is the monic polynomial with the smallest degree such that 
$m(A) = 0_{n\times n}$

\begin{exmp}
\begin{align*}
A_1 =   \left[ \begin{array}{cc} -1 & 0  \\ 0 & -1 \end{array} \right] \ \rightarrow \ d_1(\lambda) = (\lambda + 1)^2 \, , \, &\mathrm{Let} \ m_1(\lambda) = (\lambda + 1) \ \rightarrow \ m_1(A) = A + I =  \left[ \begin{array}{cc} 0 & 0  \\ 0 & 0 \end{array} \right] = 0_{n \times n} \ \checkmark
\\
\\
A_2 =   \left[ \begin{array}{cc} -1 & 1  \\ 0 & -1 \end{array} \right] \ \rightarrow \ d_2(\lambda) = (\lambda + 1)^2 \, , \, &\mathrm{Let} \ m_2(\lambda) = (\lambda + 1) \ \rightarrow \ m_1(A) = A + I =  \left[ \begin{array}{cc} 0 & 1  \\ 0 & 0 \end{array} \right] \neq 0_{n \times n} \ \text{\sffamily X}
\\
&\mathrm{Let} \ m_2(\lambda) = (\lambda + 1)^2 \ \rightarrow \ m_1(A) = (A + I)^2 =  \left[ \begin{array}{cc} 0 & 0  \\ 0 & 0 \end{array} \right] = 0_{n \times n} \ \checkmark
\end{align*}
\end{exmp}

\textbf{Theorem:} Given $A \in \mathbb{R}^{n \times n}$, let $m(\lambda)$ be its minimal polynomial 
\begin{enumerate}
	\item $m(\lambda)$ is unique
	\item $m(\lambda)$ divides $d(\lambda)$ without any reminder. $\exists q(\lambda)$ such that $d(\lambda) = q(\lambda) m(\lambda)$ 
	\item Each root of $d(\lambda)$ is a root of $m(\lambda)$, then 
\begin{align*}
m(\lambda) &= (\lambda - \lambda_1)^{m_1} \cdots (\lambda - \lambda_k)^{m_k} = \lambda^l + c_{l-1} \lambda^{l-1} + \cdots + c_0 
\\
&\mathrm{where} \ 1 \leq m_i \leq r_i \ , \ k \leq l \leq n \ , \ m_1 + \cdots + m_k = l
\\
q(\lambda) &= (\lambda - \lambda_1)^{r_1-m_1} \cdots (\lambda - \lambda_k)^{r_k-m_k} 
\end{align*}
\end{enumerate}

\textbf{Proof:} of (3) 

Since m$(\lambda)$ is the minimal polynomial, we know that $m(A) = 0_{n \times n}$. Let $(\lambda_i , v_i)$ is an eigenvalue, eigenvector pair of $A$such that $A v_i = \lambda_vi$, then
%
\begin{align*}
m(A) = 0_{n \times n} \ \rightarrow m(A) v_i &= 0_{n \times n} 
\\
\left( A^l + c_{l-1} A^{l-1} + \cdots + c_1 A + c_0 I \right) v_i &= 0_{n \times n} 
\\
\left( \lambda_i^l v_i + c_{l-1} \lambda_i^{l-1} v_i  + \cdots + c_1 \lambda_i v_i + c_0  v_i \right)  &= 0_{n \times n} 
\\
 \lambda_i^l  + c_{l-1} \lambda_i^{l-1}   + \cdots + c_1 \lambda_i  + c_0   &= 0
 \\
m(\lambda_i)  &= 0 \ \checkmark
\end{align*}

\textbf{Corollary:} Let $l$ be the order of the minimal polynomial, then the elements of $\mathcal{A}_l = \lbrace I \ , \ A \ , \ A^2 \ , \ \cdots \ , \ A^{l-1} \rbrace$ 
are linearly independent and higher order $A^i$'s can be written as a linear combination of $\lbrace I \ , \ A \ , \ A^2 \ , \ \cdots \ , \ A^{l-1} \rbrace$  

\vspace{6pt}

\textbf{Theorem:} $\mathbb{C}^{n} = \mathcal{N}\left( (\lambda_1 - A \right)^{m_1} \bigoplus \cdots \bigoplus \mathcal{N}\left( (\lambda_k I - A )^{m_k} \right) $ where $\mathrm{Dim} \left[  \mathcal{N}\left( (\lambda_i I - A )^{m_i} \right) \right] = r_i \ , \ \forall i$

\vspace{6pt}

Now let's state the necessary sufficient condition(s) such that a matrix can be diagonalizable 

\textbf{Theorem:} $A \in \mathbb{C}^{n \times n}$ is diagonalizable, if and only if 
\begin{itemize}
	\item Minimal polynomial has no repeated root. i.e.
	\begin{align*}
m(\lambda) &= (\lambda - \lambda_1)^{m_1} \cdots (\lambda - \lambda_n)^{m_k}  \ \mathrm{where} \ m_i = 1 \ \forall i 
\\  m(\lambda) &= (\lambda - \lambda_1) \cdots (\lambda - \lambda_n) 
\end{align*}
%
\item Geometric multiplicity of each eigenvalue is equal to its algebraic multiplicity, i.e.
\begin{align*}
\forall i \ \mathrm{GM}(\lambda_i) = \mathrm{dim}\left( \mathcal{N}(\lambda_i - A) \right) = r_i = \mathrm{AM}(\lambda_i) 
\end{align*}
%
\item There exist $n$ linearly independent eigenvectors of $A$
\begin{align*}
\exists \mathcal{V} = \lbrace v_1 , \cdots , v_n \rbrace, \ \mathrm{where} \ A v_i = \lambda_j \ \& \ \mathrm{Span}(\mathcal{V}) = \mathbb{C}^n
\end{align*}
\end{itemize}

\begin{exmp}
\begin{align*}
A_1 &=   \left[ \begin{array}{cc} -1 & 0  \\ 0 & -1 \end{array} \right] \ \rightarrow  \ m(\lambda) = (\lambda + 1) \, , \, 
\mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 2 = \mathrm{AM}(-1) \ , \  \mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0  \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 1  \end{array} \right]  \right\rbrace \ \checkmark
\\
\\
A_2 &=   \left[ \begin{array}{cc} -1 & 1  \\ 0 & -1 \end{array} \right] \ \rightarrow \ m(\lambda) = (\lambda + 1)^2 \, , \, 
\mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 1 \leq \mathrm{AM}(-1) \ , \  \mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0  \end{array} \right]  \right\rbrace \ \text{\sffamily X}
\end{align*}
\end{exmp}

\subsection{Computation via Jordan Form}

When a matrix $A$ is not diagonalizable, i.e. does not satisfy necessary and sufficient conditions listed above, there exists a similarity transformation, $A = G^{-1} J G$, such that $J$  
is Jordan canonical form (as close as possible to a diagonal form). Note that each $A \in \mathbb{R}^{n \times n}$ is similar to only one such J (except for a reordering of the blocks):
%
\begin{align*}
J = \left[  \begin{array}{ccccc} J_1 & \mathbf{0} & \cdots & & \mathbf{0} \\ \mathbf{0} & J_2 & \mathbf{0} & \cdots & \mathbf{0} \\ \vdots &  & \ddots &  \\ & & & \ddots &  \\
\mathbf{0} &  &  &  & J_{N_J} \end{array} \right] \ \mathrm{where} \ 
J_i = \left[  \begin{array}{ccccc} \lambda_i & 1 & 0 & \cdots & 0  \\ 0 & \lambda_i & 1 & 0 & \cdots  \\ 
\vdots &  & \ddots &  \\ & & & \ddots & 1 \\
0 &  &  & 0 & \lambda_i \end{array} \right]  \in \mathbb{C}^{n_i \times n_i}
\end{align*}

\begin{exmp}
\begin{align*}
A_1 &=   \left[ \begin{array}{cccc} -1 & 0 & 0 & 0  \\ 0 & -1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -1 \end{array} \right] \ \rightarrow  
\begin{array}{c} \ m(\lambda) = (\lambda + 1) \\ \mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 4 
\\
\mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0  \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 1 \\ 0 \\ 0   \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 1 \\ 0   \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 0 \\ 1   \end{array} \right] \right\rbrace 
\end{array}
\\
\\
A_2 &=   \left[ \begin{array}{cccc} -1 & 1 & 0 & 0  \\ 0 & -1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -1 \end{array} \right] \ \rightarrow  
\begin{array}{c} \ m(\lambda) = (\lambda + 1)^2 \\ \mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 3 
\\
\mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0  \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 1 \\ 0   \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 0 \\ 1   \end{array} \right] \right\rbrace 
\end{array}
\\
\\
A_3 &=   \left[ \begin{array}{cccc} -1 & 1 & 0 & 0  \\ 0 & -1 & 0 & 0 \\ 0 & 0 & -1 & 1 \\ 0 & 0 & 0 & -1 \end{array} \right] \ \rightarrow  
\begin{array}{c} \ m(\lambda) = (\lambda + 1)^2 \\ \mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 2 
\\
\mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0  \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 1 \\ 0   \end{array} \right]  \right\rbrace 
\end{array}
\\
\\
A_4 &=   \left[ \begin{array}{cccc} -1 & 1 & 0 & 0  \\ 0 & -1 & 1 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -1 \end{array} \right] \ \rightarrow  
\begin{array}{c} \ m(\lambda) = (\lambda + 1)^3 \\ \mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 2 
\\
\mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0  \end{array} \right] ,   \left[ \begin{array}{c} 0  \\ 0 \\ 0 \\ 1 \end{array} \right]  \right\rbrace 
\end{array}
\\
\\
A_5 &=   \left[ \begin{array}{cccc} -1 & 1 & 0 & 0  \\ 0 & -1 & 1 & 0 \\ 0 & 0 & -1 & 1 \\ 0 & 0 & 0 & -1 \end{array} \right] \ \rightarrow  
\begin{array}{c} \ m(\lambda) = (\lambda + 1)^4 \\ \mathrm{GM}(-1) = \mathrm{dim}\left( \mathcal{N}((-1) - A) \right) = 1
\\
\mathcal{V} = \left\lbrace  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0  \end{array} \right]   \right\rbrace 
\end{array}
\end{align*}
\end{exmp}

\textbf{Proposition:} Jordan transformation of a matrix $A \in \mathbb{C}^{n \times n}$ with 
$d(\lambda) = (\lambda - \lambda_1)^{r_1} \cdots (\lambda - \lambda_n)^{r_k} $ and
$m(\lambda) = (\lambda - \lambda_1)^{m_1} \cdots (\lambda - \lambda_n)^{m_k} $, for each $\lambda_i$
%
\begin{itemize}
	\item $m_i :$ Size of the largest Jordan block for $\lambda_i$
	\item $\mathrm{GM}(\lambda) = \mathrm{dim}\left( \mathcal{N}(\lambda_i - A) \right) : $ \# Jordan blocks for $\lambda_i$
	\item $\mathrm{dim}\left( \mathcal{N}(\lambda_i - A)^k \right) - \mathrm{dim}\left( \mathcal{N}(\lambda_i - A)^{k-1} \right) : $ \# Jordan blocks for $\lambda_i$ with size $\geq k$
\end{itemize}

\subsubsection{Construction of $G$ for $A = GJ G^{-1} $}

If $A$ is a diagonalizable matrix transformation matrices, $V \, \& V^{-1}$, are simply composed of right and left eigenvectors 
%
\begin{align*}
 V &= \left[ \begin{array}{ccc} v_1 & \cdots & v_n \end{array} \right] \ , \ \mathrm{where} \ A v_i = \lambda_i v_i 
 \\
 V^{-1} &= \left[ \begin{array}{c} \bar{v}_1^T \\ \vdots \\ \bar{v}_n^T \end{array} \right] \ , \ \mathrm{where} \ \bar{v}_i^T A = \lambda_i \bar{v}_i^T
  \\
 V^{-1} V &= I \ , \ \bar{v}_i^T v_i = 1 \ , \  \bar{v}_i^T v_j = 0 \ \mathrm{for} \, i \neq j
\end{align*}
%
When the matrix, $A \in \mathbb{C}^{n \times n}$, is not diagonalizable, i.e. does not have $n$ linearly independent eigenvectors to
construct the transformation matrix, $G$, we need to add (special) linearly independent vectors to complete the transformation matrix, 
such that  $A = G J G^{-1}$. These vectors are generated from the eigenvectors and are called \textit{generalized eigenvec­tors} of $A$.
%
Suppose $(\lambda , g_1)$ is an eigenvalue--eigenvector pair of $A$ associated with Jordan block of size $k$. $k-1$ generalized eigenvec­tors, 
$\lbrace g_2 \ , \ \cdots \ , \ g_{k} \rbrace$, are constructed as follows 
%
\begin{align*}
	A g_1 &= \lambda g_1 \ \rightarrow \ (A - \lambda I) g_1 = 0
	\\
	A g_2 &= \lambda g_2 + g_1 \ \rightarrow \ (A - \lambda I) g_2 = g_1 \ , \ \mathrm{note} \ (A - \lambda I)^2 g_2 = 0 \ \& \ (A - \lambda I) g_2 \neq 0
		\\
	A g_3 &= \lambda g_3 + g_2 \ \rightarrow \ (A - \lambda I) g_3 = g_2 \ , \ \mathrm{note} \ (A - \lambda I)^3 g_3 = 0 \ \& \ (A - \lambda I)^2 g_3 \neq 0
	\\
	&\vdots
	\\
	A g_{k} &= \lambda g_{k} + g_{k-1} \ \rightarrow \ (A - \lambda I) g_k = g_{k-1} \ , \ \mathrm{note} \ (A - \lambda I)^k g_k = 0 \ \& \ (A - \lambda I)^{k-1} g_{k} \neq 0
\end{align*}
%
The string $\left[ \begin{array}{cccc}  g_1 & g_2 & \cdots & g_k \end{array} \right]$ is called a Jordan chain. We can also re-write the relation between 
the generalized eigenvectors in matrix form using the Jordan chain
%
\begin{align*}
	A \left[ \begin{array}{cccc}  g_1 & g_2 & \cdots & g_k \end{array} \right] = \left[ \begin{array}{cccc}  g_1 & g_2 & \cdots & g_k \end{array} \right] 
	\left[  \begin{array}{ccccc} \lambda & 1 & 0 & \cdots & 0  \\ 0 & \lambda & 1 & 0 & \cdots  \\ 
\vdots &  & \ddots &  \\ & & & \ddots & 1 \\
0 &  &  & 0 & \lambda \end{array} \right]
\end{align*}

\begin{exmp}
	Let 
\begin{align*}
A = \left[ \begin{array}{ccc} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1
\end{array} \right] , 
\end{align*}
\end{exmp}
%
compute $d(\lambda)$, $m(\lambda)$, algebraic multiplicity eigenvalues, geometric multiplicity eigenvalues, Jordan canonical form, and transformation matrix, $G$.

\textbf{Solution:} Let's start with  $d(\lambda)$
%
\begin{align*}
  \lambda I - A = \left[ \begin{array}{ccc} \lambda - 1 & -1 & -1 \\ 0 & \lambda - 1 & -1 \\ 0 & 0 & \lambda - 1 \end{array} \right] 
  \ \rightarrow \ d(\lambda) = (\lambda - 1)^3 \ , \ AM(1) = 3
\end{align*}
% 
Let's find $m(\lambda)$ and associated features
%
%
\begin{align*}
  \Sigma &= \left[ A - \lambda \right]_{\lambda = 1} = \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] 
  \\
  \Sigma^2 &= \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right]  \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] = \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array} \right]
  \\
  \Sigma^3 &= \left[ \begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array} \right] \ \rightarrow \ m(\lambda) = (\lambda - 1)^3 \ , \ 
  \mathrm{GM}(1) = 1 \ , \ J = \left[ \begin{array}{ccc} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{array} \right] 
\end{align*}
% 
Now let's compute the eigenvector and associated generalized eigenvectors
%
\begin{align*}
 \left[ A - \lambda \right]_{\lambda = 1} g_1 = \Sigma g_1 = 0 \ \rightarrow \ 
 \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] g_1 = 0 \ \rightarrow \ g_1 =  
 \left[ \begin{array}{c} 1 \\ 0  \\ 0  \end{array} \right]
 \\
  \left[ A - \lambda \right]_{\lambda = 1} g_2 = g_1 \ \rightarrow \
  \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] g_2 =  \left[ \begin{array}{c} 1 \\ 0  \\ 0  \end{array} \right] \ \rightarrow \ 
  g_2 =   \left[ \begin{array}{c} 0 \\ 1  \\ 0  \end{array} \right]
  \\
  \left[ A - \lambda \right]_{\lambda = 1} g_3 = g_1 \ \rightarrow \
  \left[ \begin{array}{ccc} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] g_3 =  \left[ \begin{array}{c} 0 \\ 1  \\ 0  \end{array} \right] \ \rightarrow \ 
  g_2 =   \left[ \begin{array}{c} 0 \\ -1  \\ 1  \end{array} \right]
\end{align*}
%
Thus we can construct the transformation matrix and its inverse as
\begin{align*}
 G &= \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & -1  \\ 0 & 0 & 1  \end{array} \right]
 \  , \ 
 G^{-1} = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 1  \\ 0 & 0 & 1  \end{array} \right]
\end{align*}

\begin{exmp}
	Let 
\begin{align*}
A = \left[ \begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1
\end{array} \right] , 
\end{align*}
\end{exmp}
%
compute $d(\lambda)$, $m(\lambda)$, algebraic multiplicity eigenvalues, geometric multiplicity eigenvalues, Jordan canonical form, and transformation matrix, $G$.

\textbf{Solution:} Let's start with  $d(\lambda)$
%
\begin{align*}
  \lambda I - A = \left[ \begin{array}{ccc} \lambda - 1 & 0 & -1 \\ 0 & \lambda - 1 & -1 \\ 0 & 0 & \lambda - 1 \end{array} \right] 
  \ \rightarrow \ d(\lambda) = (\lambda - 1)^3 \ , \ AM(1) = 3
\end{align*}
% 
Let's find $m(\lambda)$ and associated features
%
%
\begin{align*}
  \Sigma &= \left[ A - \lambda \right]_{\lambda = 1} = \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] 
  \\
  \Sigma^2 &= \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right]  \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] = \left[ \begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array} \right]
\\
  m(\lambda) &= (\lambda - 1)^2 \ , \ \mathrm{GM}(1) = 2 \ , \ J = \left[ \begin{array}{ccc} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right] 
\end{align*}
% 
First compute the eigenvectors and eigenspace 
%
\begin{align*}
 \left[ A - \lambda \right]_{\lambda = 1} v = \Sigma v = 0 \ \rightarrow \ 
 \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] v = 0 \ \rightarrow \ \mathcal{V} = \mathrm{Span} \left( \lbrace x , y \rbrace  \right)
= \mathrm{Span} \left( \left\lbrace \left[ \begin{array}{c} 1 \\ 0  \\ 0  \end{array} \right] ,  \left[ \begin{array}{c} 0 \\ 1  \\ 0  \end{array} \right] \right\rbrace \right)
\end{align*}
%
%
Now let's find the generalized eigenvectors
%
\begin{align*}
&\left[ A - \lambda \right]_{\lambda = 1} g_2 = g_1 \ \mathrm{where} \ g_1 \in \mathrm{Span} \left( \lbrace x , y \rbrace  \right)
\ \rightarrow \ \Sigma g_2 = c_1 x + c_2 y
\\
 & \left[ \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] g_2 =  \left[ \begin{array}{c} c_1 \\ c_2 \\ 0  \end{array} \right] \ \rightarrow \ 
  g_2 =   \left[ \begin{array}{c} 0 \\ 0  \\ 1 \end{array} \right] \ \rightarrow \  g_1 = \Sigma g_2 =  \left[ \begin{array}{c} 1 \\ 1  \\ 0 \end{array} \right] 
\end{align*}
%
Thus we can construct a transformation matrix and its inverse as
\begin{align*}
 G &= \left[ \begin{array}{cccc} g_1 & g_2 & x  \end{array} \right]
 \ , \ 
 G = \left[ \begin{array}{ccc} 1 & 0 & 1 \\ 1 & 0 & 0  \\ 0 & 1 & 0  \end{array} \right]
 \  , \ 
 G^{-1} = \left[ \begin{array}{ccc} 0 & 1 & 0 \\ 0 & 0 & 1  \\ 1 & -1 & 0 \end{array} \right]
\end{align*}

\subsubsection{Computation $J(\lambda)^k$ and $ e^{J(\lambda) t} $}

First let's focus on the powers of $J(0)$
%
\begin{align*}
J(0) &= \left[  \begin{array}{cccccc} 0 & 1 & 0 & \cdots & 0 & 0  \\ 0 & 0 & 1 & 0 & \cdots  & 0 \\ 
\vdots &  & \ddots & \ddots &  \\ & & & \ddots & 1 & 0 \\ 0 &  & \cdots  &  & 0 & 1 \\
0 &  & \cdots &  & 0 & 0 \end{array} \right]  \ , \ 
J(0)^2 = \left[  \begin{array}{cccccc} 0 & 0 & 1 & \cdots & 0 & 0  
\\ 0 & 0 & 0 & 1 & \cdots  & 0 \\ 
\vdots &  & \ddots & \ddots &  \\ & & & \ddots & 0 & 1 \\ 0 &  & \cdots  &  & 0 & 0 \\
0 &  & \cdots &  & 0 & 0 \end{array} \right] , 
\\
J(0)^{n-2} &= \left[  \begin{array}{cccccc} 0 & 0 & 0 & \cdots & 1 & 0  
\\ 0 & 0 & 0 & 0 & \cdots  & 1 \\ 
\vdots &  & \ddots & \ddots &  \\ & & & \ddots & 0 & 0 \\ 0 &  & \cdots  &  & 0 & 0 \\
0 &  & \cdots &  & 0 & 0 \end{array} \right] 
\ , \ 
J(0)^{n-1} = \left[  \begin{array}{cccccc} 0 & 0 & 0 & \cdots & 0 & 1  
\\ 0 & 0 & 0 & 0 & \cdots  & 0 \\ 
\vdots &  & \ddots & \ddots &  \\ & & & \ddots & 0 & 0 \\ 0 &  & \cdots  &  & 0 & 0 \\
0 &  & \cdots &  & 0 & 0 \end{array} \right] \ , \ J(0)^n = \mathbf{0}_{n \times n}
\end{align*}
%
Note that $J(0)^i = \mathbf{0}_{n \times n}$ for $i \geq n$. Now let's expand 
$J(\lambda)^k = \left( \lambda I +  J(0)\right)^k$ using binomial theorem (note that $I$ and $J(0)$ commutes)
%
\begin{align*}
	J(\lambda)^k = \lambda^k I + \left( \begin{array}{c} k \\ 1 \end{array} \right) \lambda^{k-1} J(0) + 
	\left( \begin{array}{c} k \\ 2 \end{array} \right) \lambda^{k-2} J(0)^2 + \cdots + 
	\left( \begin{array}{c} k \\ n-1 \end{array} \right) \lambda^{k-n+1} J(0)^{n-1} 
\end{align*}
% 
note that by definition $\left( \begin{array}{c} k \\ i \end{array} \right) = 0 $ if $i > k$, thus above formulation is also valid for $k < n$. If we combine 
all elements in single matrix we obtain
%
\begin{align*}
J(0) &= \left[  \begin{array}{cccccc} \lambda^k & \left( \begin{array}{c} k \\ 1 \end{array} \right) \lambda^{k-1} & \left( \begin{array}{c} k \\ 2 \end{array} \right) \lambda^{k-2} & \cdots & 0 & 0  
\\ 0 & \lambda^k & \left( \begin{array}{c} k \\ 1 \end{array} \right) \lambda^{k-1} & 0 & \cdots  & 0 \\ 
\vdots &  & \ddots & \ddots &  \\ & & & \ddots & \left( \begin{array}{c} k \\ 1 \end{array} \right) \lambda^{k-1} & \left( \begin{array}{c} k \\ 2 \end{array} \right) \lambda^{k-2} \\ 0 &  & \cdots  &  & \lambda^k & \left( \begin{array}{c} k \\ 1 \end{array} \right) \lambda^{k-1} \\
0 &  & \cdots &  & 0 & \lambda^k \end{array} \right] 
\end{align*}



% **** This ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:
\end{document}
